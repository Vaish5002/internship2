<h4>1. Import ipywidgets as widgets</h4>
<pre>
import ipywidgets as widgets
text_widget = widgets.Text(value='Initial value', placeholder='Type something', description='String:')
button_widget = widgets.Button(description='Check me!')
color_picker_widget = widgets.ColorPicker(description='Pick a color:', value='blue')
int_slider_widget = widgets.IntSlider(min=0, max=10, description='Intensity:')
display(text_widget, button_widget, color_picker_widget, int_slider_widget)
</pre>

<h4>1.2. Import ipywidgets as widgets from IPython.display</h4>
<pre>
import ipywidgets as widgets
from IPython.display import display, clear_output
num1 = widgets.IntText(value=0, description='Number 1:')
num2 = widgets.IntText(value=0, description='Number 2:')
output = widgets.Output()
display(num1, num2, output)

def update_output(change):
    with output:
        clear_output()
        result = num1.value + num2.value
        print(f'The sum is: {result}')

num1.observe(update_output)
num2.observe(update_output)
</pre>

<h4>2. NumPy Array Operations</h4>
<pre>
import numpy as np
array = np.array([1,2,3,4,5,6,7,8,9,1])
mean = np.mean(array)
sum = np.sum(array)
product = np.prod(array)
variance = np.var(array)
std = np.std(array)
print("Mean: ", mean)
print("sum:", sum)
print("product:", product)
print("variance:", variance)
print("standard deviation:", std)
</pre>

<h4>2.2. Sorting a NumPy Array</h4>
<pre>
import numpy as np
array = np.array([64,32,25,12,22,11,9,8])
sorted_array = np.sort(array)
print("original array:", array)
print("sorted array:", sorted_array)
</pre>

<h4>2.3. Slicing a NumPy Array</h4>
<pre>
import numpy as np
array = np.array([[1,2,3],[4,5,6],[7,8,9]])
print("original array")
print(array)
print("\n slicing rows 0 and 1:")
print(array[0:2,:])
print("\n slicing column:")
print(array[:,1])
</pre>

<h4>3. Pandas DataFrame with Missing Values</h4>
<pre>
import pandas as pd
import numpy as np
data = np.random.randint(0,100,size=(6,6))
df = pd.DataFrame(data, columns=['a','b','c','d','e','f'])
for i in range(6):
    df.loc[i, chr(97+i)] = np.nan
print("Data frame with missing values")
print(df)
</pre>

<h4>3.2. Dropping Rows with Missing Values</h4>
<pre>
import pandas as pd
import numpy as np
data = np.random.randint(0,100,size=(10,6))
df = pd.DataFrame(data, columns=['a','b','c','d','e','f'])
for i in range(6):
    df.loc[i, chr(97+i)] = np.nan
print("Data frame with missing values")
print(df)
print("\n data frame after dropping rows without any missing values")
print(df.dropna())
</pre>

<h4>3.3. Filling Missing Values in Pandas DataFrame</h4>
<pre>
import pandas as pd
import numpy as np
data = np.random.randint(0,100,size=(10,6))
newd = np.random.randint(0,100,size=(10,6))
df = pd.DataFrame(data, columns=['a','b','c','d','e','f'])
other = pd.DataFrame(newd, columns=['a','b','c','d','e','f'])
for i in range(6):
    df.loc[i, chr(97+i)] = np.nan
print("Data frame with missing values")
print(df)
print("Filling values with previous values")
print(df.ffill())
print("Filling nan values with next values")
print(df.bfill())
print("\n filling values with another data frame:")
print(df.fillna(other))
</pre>

<h4>3.4. Dropping Rows with Missing Values for a Given Subset</h4>
<pre>
import pandas as pd
import numpy as np
data = np.random.randint(0,100,size=(10,6))
df = pd.DataFrame(data, columns=['a','b','c','d','e','f'])
for i in range(6):
    df.loc[i, chr(97+i)] = np.nan
print("Data frame with missing values")
print(df)
print("\n data frame after dropping rows without any missing values for a given subset")
print(df.dropna(subset=["a","b"]))
</pre>

<h4>4. Curve Fitting with SciPy</h4>
<pre>
import numpy as np
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

def func(x, a, b, c):
    return a * np.sin(b * x) + c

xdata = np.linspace(0, 4 * np.pi, 50)
y = func(xdata, 2.5, 1.3, 0.5)
ydata = y + 0.1 * np.random.normal(size=len(xdata))

popt, p = curve_fit(func, xdata, ydata)

residuals = ydata - func(xdata, *popt)
variance = np.var(residuals)

print("Coefficients:", popt)
print("Variance:", variance)

plt.plot(xdata, ydata, 'ko', label="Original data")
plt.plot(xdata, func(xdata, *popt), 'r-', label="Fitted curve")
plt.legend()
plt.show()
</pre>

<h4>5. Linear Regression with Scikit-learn</h4>
<pre>
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

np.random.seed(0)
x = 2 * np.random.rand(100, 1)
y = 4 + 3 * x + np.random.rand(100, 1)

model = LinearRegression()
model.fit(x, y)
x_new = np.array([[0], [2]])
y_predict = model.predict(x_new)

plt.plot(x_new, y_predict, "r-", label="Prediction")
plt.plot(x, y, "b.", label="Data")
plt.xlabel("x")
plt.ylabel("y")
plt.title("Linear Regression")
plt.legend()
plt.show()
</pre>

<h4>5.2. Polynomial Regression</h4>
<pre>
  import numpy as np
  import matplotlib.pyplot as plt
  np.random.seed(0)
  x = np.linspace(0, 10, 20)
  y = 2 * x**2 + 3 * x + 5 + np.random.normal(scale=5, size=len(x))
  degree = 2
  coefficients = np.polyfit(x, y, degree)
  polynomial = np.poly1d(coefficients)
  x_fit = np.linspace(0, 10, 100)
  y_fit = polynomial(x_fit)
  plt.scatter(x, y, label="Original Data")
  plt.plot(x_fit, y_fit, color='red', label="Fitted Polynomial")
  plt.xlabel("x")
  plt.ylabel("y")
  plt.title(f"Polynomial Regression (Degree {degree})")
  plt.legend()
  plt.show()
  print(f"Coefficients: {coefficients}")
</pre>

<h4>5.3. Exponential Regression</h4>
<pre>
  import numpy as np
  from scipy.optimize import curve_fit
  import matplotlib.pyplot as plt
  def exponential_func(x, a, b, c):
      return a * np.exp(b * x) + c
  np.random.seed(0)
  x = np.linspace(0, 5, 20)
  y = exponential_func(x, 2, 1.5, 1) + np.random.normal(scale=0.5, size=len(x))
  popt, pcov = curve_fit(exponential_func, x, y)
  print(f"Fitted parameters: a={popt[0]:.2f}, b={popt[1]:.2f}, c={popt[2]:.2f}")
  plt.scatter(x, y, label="Original Data")
  plt.plot(x, exponential_func(x, *popt), color='red', label="Fitted Curve")
  plt.xlabel("x")
  plt.ylabel("y")
  plt.title("Exponential Regression")
  plt.legend()
  plt.show()
</pre>

<h4>6. K-Nearest Neighbors Classifier</h4>
<pre>
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

X, y = make_blobs(centers=2, random_state=0)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X, y)

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')
plt.show()
</pre>

<h4>7. Boxplot for Outlier Detection</h4>
<pre>
import numpy as np
import matplotlib.pyplot as plt

# Create sample data
data = np.random.randint(1, 20, size=100)

# Calculate IQR
q1, q3 = np.percentile(data, [25, 75])
iqr = q3 - q1
lower_bound = q1 - (1.5 * iqr)
upper_bound = q3 + (1.5 * iqr)

# Find outliers
outliers = data[(data < lower_bound) | (data > upper_bound)]

# Plot boxplot
plt.boxplot(data)
plt.show()
</pre>

<h4>8.a. Histogram</h4>
<pre>
import numpy as np
import matplotlib.pyplot as plt

# Set the style for the plot (optional)
plt.style.use('ggplot')

# Define parameters for the data
mu, sigma, size = 0, 1, 1000  # Mean, standard deviation, and size of the array
# Generate random data from a normal distribution
data = np.random.normal(mu, sigma, size)

# Create the histogram
fig, ax = plt.subplots()
ax.hist(data, bins=30, density=True, alpha=0.7, color='b', label='Histogram')

# Add labels and title
ax.set_xlabel('Value')
ax.set_ylabel('Probability Density')
ax.set_title('Histogram of Normal Distribution')

# Show the plot
plt.legend()
plt.show()
</pre>

<h4>8.b. Bar Chart</h4>
<pre>
import numpy as np
import matplotlib.pyplot as plt

# Sample data
data = np.array([10, 15, 12, 8, 18])

# Calculate average and mean
average = np.mean(data)
mean = np.median(data)

# Create arrays for the plot
x = np.arange(len(data))
width = 0.8

# Build the plot
plt.bar(x, data, width, color='blue', label='Data')
plt.axhline(y=average, color='red', linestyle='--', label='Average')
plt.axhline(y=mean, color='green', linestyle='-', label='Mean')

# Add labels and title
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Bar Chart')
plt.xticks(x, ['A', 'B', 'C', 'D', 'E'])  # Customize x-axis labels
plt.legend()

# Display the plot
plt.show()
</pre>

<h4>8.c. Pie Chart</h4>
<pre>
import matplotlib.pyplot as plt

# Sample data
labels = ['A', 'B', 'C', 'D']
sizes = [15, 30, 45, 10]
colors = ['lightcoral', 'yellowgreen', 'lightskyblue', 'gold']
explode = (0, 0.1, 0, 0)  # only "explode" the 2nd slice (i.e. 'Hogs')

# Plot the pie chart
fig1, ax1 = plt.subplots()
ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle

# Display the plot
plt.show()
</pre>

<h4>9. K-means Clustering</h4>
<pre>
from sklearn.datasets import make_blobs
import numpy as np
import matplotlib.pyplot as plt

def kmeans(X, k, max_iter=100):
    centroids = X[np.random.choice(len(X), k)]
    for _ in range(max_iter):
        labels = np.argmin(np.linalg.norm(X[:, None] - centroids, axis=2), axis=1)
        centroids = np.array([X[labels == i].mean(0) for i in range(k)])
    return labels

X, _ = make_blobs(n_samples=300, centers=3)
labels = kmeans(X, 3)

plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.show()
</pre>

<h4>10.a. Sine Wave Plot</h4>
<pre>
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 100)
y = np.sin(x)

plt.plot(x, y)
plt.title('Sine Wave')
plt.xlabel('x')
plt.ylabel('Sine(x)')
plt.grid(True)
plt.show()
</pre>

<h4>10.b. 3D Surface Plot</h4>
<pre>
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
x = np.linspace(-1, 1, 100)
y = np.linspace(-1, 1, 100)
X, Y = np.meshgrid(x, y)
z = 2*X + 2*Y

ax.plot_trisurf(X, Y, z, cmap='viridis')
ax.set_title('3D Surface Plot')
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
plt.show()
</pre>
